{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import glob\n",
    "import pandas as pd\n",
    "import re\n",
    "import itertools\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import json\n",
    "import ntpath\n",
    "\n",
    "def feature_to_list(s, key):\n",
    "    if key not in {\"sentid\", \"docid\"}:\n",
    "        s = s.replace(\"{\",\"\").replace(\"}\",\"\")\n",
    "        s = \"[\" + s + \"]\"\n",
    "        res = eval(s)\n",
    "\n",
    "        if key == \"wordidx\":\n",
    "            res = [int(x) for x in res]\n",
    "    else:\n",
    "        res = s\n",
    "        \n",
    "    return(res)\n",
    "\n",
    "def sentence_to_dict(i, sentence, fpath):\n",
    "    row = dict()\n",
    "    dep = sorted(sentence[\"basicDependencies\"], key = lambda x: x[\"dependent\"])\n",
    "    \n",
    "    row[\"dep_paths\"] = [x[\"dep\"] for x in dep]\n",
    "    row[\"dep_parents\"] = [x[\"governor\"] for x in dep]\n",
    "    row[\"word\"] = [x[\"word\"] for x in sentence[\"tokens\"]]\n",
    "    row[\"ners\"] = [x[\"ner\"] for x in sentence[\"tokens\"]]\n",
    "    row[\"poses\"] = [x[\"pos\"] for x in sentence[\"tokens\"]]\n",
    "    row[\"wordidx\"] = [x[\"index\"] for x in sentence[\"tokens\"]]\n",
    "    row[\"lemmas\"] = [x[\"lemma\"] for x in sentence[\"tokens\"]]\n",
    "\n",
    "    row[\"docid\"] = ntpath.basename(fpath)\n",
    "    row[\"sentid\"] = i+1\n",
    "    \n",
    "    return(row)\n",
    "\n",
    "def index_to_tokens(x, row):\n",
    "    start = x[0]\n",
    "    end = start + len(x)\n",
    "    return(row[\"word\"][start:end])\n",
    "\n",
    "def consecutive(data, stepsize=1):\n",
    "    res = np.split(data, np.where(np.diff(data) != stepsize)[0]+1)\n",
    "    res = np.array(res)\n",
    "    return(res)\n",
    "\n",
    "def tokens_nonconsecutive_ner(word, ners, label = \"TAXA\"):  \n",
    "    tagged = [i for i,(w, ner) in enumerate(zip(word, ners)) if ner == label]\n",
    "\n",
    "    ner_label_indices = consecutive(tagged)\n",
    "    \n",
    "    tagged_offset = tagged[1:]\n",
    "    tagged_offset.append(None)\n",
    "    \n",
    "    w = np.array(word)\n",
    "    targets = [w[i] for i in ner_label_indices]\n",
    "    \n",
    "    entities = []\n",
    "    \n",
    "    for i, (idx, target) in enumerate(zip(ner_label_indices, targets)):\n",
    "        res = dict()\n",
    "        res[\"idx\"] = idx.tolist()\n",
    "        res[label] = w[idx].tolist()\n",
    "        entities.append(res)\n",
    "\n",
    "    return(entities)\n",
    "\n",
    "pattern_genus = re.compile(\"[A-Z][.]\")\n",
    "\n",
    "class smart_dict(dict):\n",
    "    def __missing__(self, key):\n",
    "        return(key)\n",
    "\n",
    "def obtain_candidates(df, span = \"INTERVALNAME\"):\n",
    "    candidates = []\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        if len({\"TAXA\", span}.intersection(set(row[\"ners\"]))) > 1 and len(row[\"word\"]) < 70:\n",
    "            taxa = tokens_nonconsecutive_ner(row[\"word\"], row[\"ners\"], \"TAXA\")\n",
    "            intervals = tokens_nonconsecutive_ner(row[\"word\"], row[\"ners\"], span)\n",
    "\n",
    "            #Deabbreviate genus names\n",
    "            is_taxa = np.array(row[\"ners\"]) == \"TAXA\"\n",
    "            is_abbrev = np.array([True if pattern_genus.match(x) else False for x in row[\"word\"]])\n",
    "\n",
    "            abbrevs = set(np.array(row[\"word\"])[is_taxa & is_abbrev])\n",
    "\n",
    "            if abbrevs:\n",
    "                d = smart_dict()\n",
    "                for abbrev in abbrevs:\n",
    "                    d[abbrev] = replace_abbrev(abbrev, df, i, 8)\n",
    "                \n",
    "                for entity in taxa:\n",
    "                    for k, v in d.items():\n",
    "                        entity[\"TAXA\"] = [v if x == k else x for x in entity[\"TAXA\"]]\n",
    "            \n",
    "            ## set up dependency tree in networkx\n",
    "            G = nx.Graph()\n",
    "\n",
    "            dep = row[\"dep_parents\"]\n",
    "            nodes = row[\"wordidx\"]\n",
    "            parents = [int(x) for x in dep]\n",
    "            edges = zip(nodes, parents)\n",
    "\n",
    "            G.add_edges_from(edges)\n",
    "\n",
    "            ## All combinations of the spans (product)\n",
    "            for p in itertools.product(taxa, intervals):\n",
    "                sdp = dict()\n",
    "                a, b = sorted([p[0], p[1]], key = lambda x: x[\"idx\"])\n",
    "                \n",
    "                try:\n",
    "                    sdp[\"idx\"] = nx.shortest_path(G, a[\"idx\"][-1]+1, b[\"idx\"][0]+1)\n",
    "                    #sdp[\"idx\"] = nx.shortest_path(G, a[\"idx\"][-1], b[\"idx\"][0])\n",
    "                except:\n",
    "                    print(row[\"docid\"])\n",
    "                    print(row[\"sentid\"])\n",
    "\n",
    "                    raise Exception(\"Could not compute SPD\")\n",
    "                    \n",
    "                sdp[\"words\"] = [row[\"word\"][i-1] for i in sdp[\"idx\"]]\n",
    "\n",
    "                ## Compute SPD for each\n",
    "                candidate = dict()\n",
    "\n",
    "                candidate[\"sdp\"] = sdp\n",
    "\n",
    "                candidate[\"TAXA\"] = p[0]\n",
    "                candidate[span] = p[1]\n",
    "                candidate[\"sentid\"] = int(i)\n",
    "                candidate[\"sentence\"] = row[\"word\"]\n",
    "                candidate[\"docid\"] = row[\"docid\"].replace(\".json\", \"\")\n",
    "\n",
    "                candidates.append(candidate)\n",
    "\n",
    "    return(candidates)\n",
    "\n",
    "def replace_abbrev(abbrev, df, index, count):\n",
    "    previous_words = df.iloc[index][\"word\"]\n",
    "    prev_ner = df.iloc[index][\"ners\"]    \n",
    "\n",
    "    genus_toks = [x for i, x in enumerate(previous_words) if x.startswith(abbrev[0]) and len(x) > 2 and prev_ner[i] == \"TAXA\"]\n",
    "    \n",
    "    if genus_toks:\n",
    "        replacement = genus_toks[-1]\n",
    "    else:\n",
    "        if int(count) > 0:\n",
    "            replacement = replace_abbrev(abbrev, df, index, count -1)\n",
    "        else:\n",
    "            replacement = abbrev\n",
    "    return(replacement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'INTERVALNAME', 'TAXA'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#{\"TAXA\", span}.intersection({\"INTERVALNAME\", \"TAXA\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import json\n",
    "\n",
    "#fpaths = glob.glob(\"data/nlp390_bryozoa/*\")\n",
    "fpaths = glob.glob(\"data/*.tsv\")\n",
    "#fpaths = glob.glob(\"/home/storage/corenlp_wrapper/output/*/*.json\")[0:2]\n",
    "fpaths = \n",
    "\n",
    "\n",
    "## Read the csv files\n",
    "\n",
    "header = [\"docid\", \"sentid\", \"wordidx\", \"word\", \"poses\", \"ners\", \"lemmas\", \"dep_paths\", \"dep_parents\"]\n",
    "\n",
    "for fpath in tqdm.tqdm(fpaths):\n",
    "    if \".json\" in fpath:  \n",
    "        l = []\n",
    "\n",
    "        with open(fpath, \"r\") as f:\n",
    "            raw = f.read()\n",
    "            if not raw or \"java.util\" in raw:\n",
    "                continue ## Skip current iteration in loop\n",
    "            else:\n",
    "                s = json.loads(raw)\n",
    "        sentences = s[0][\"sentences\"]\n",
    "\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            l.append(sentence_to_dict(i, sentence, fpath))\n",
    "\n",
    "        df = pd.DataFrame(l)\n",
    "\n",
    "    else:\n",
    "        df = pd.read_csv(fpath, header=None, names = header, sep =\"\\t\")\n",
    "\n",
    "        for k, v in df.iteritems():\n",
    "            df[k] = [feature_to_list(x, k) for x in v]\n",
    "                \n",
    "\n",
    "\n",
    "    candidates = obtain_candidates(df)\n",
    "\n",
    "    if candidates:\n",
    "        gddid = candidates[0][\"docid\"]\n",
    "\n",
    "\n",
    "        with open(\"output/{}.json\".format(gddid), \"w\") as f:\n",
    "            json.dump(candidates, f, indent=4, sort_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "docid                                   5ab3d0c3cf58f10e4a1e0ea5\n",
       "sentid                                                       374\n",
       "wordidx        [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...\n",
       "word           [Taxanomic, status, of, the, cyclostomes, bryo...\n",
       "poses          [JJ, NN, IN, DT, NNS, NNS, VBZ, NNP, ,, IN, IN...\n",
       "ners           [O, O, O, O, O, O, O, TAXA, O, O, O, O, O, O, ...\n",
       "lemmas         [taxanomic, status, of, the, cyclostome, bryoz...\n",
       "dep_paths      [amod, nsubj, case, det, compound, nmod, , dob...\n",
       "dep_parents    [2, 7, 6, 6, 6, 2, 0, 7, 8, 13, 13, 13, 24, 16...\n",
       "Name: 373, dtype: object"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row = df.iloc[373]\n",
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "span = \"INTERVALNAME\"\n",
    "\n",
    "\n",
    "if len({\"TAXA\", span}.intersection(set(row[\"ners\"]))) > 1 and len(row[\"word\"]) < 70:\n",
    "    taxa = tokens_nonconsecutive_ner(row[\"word\"], row[\"ners\"], \"TAXA\")\n",
    "    intervals = tokens_nonconsecutive_ner(row[\"word\"], row[\"ners\"], span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'INTERVALNAME': ['Upper', 'Cretaceous'], 'idx': [22, 23]}]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'TAXA': ['Exidmonea'], 'idx': [7]},\n",
       " {'TAXA': ['E.', 'dorsata'], 'idx': [14, 15]}]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taxa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'INTERVALNAME': {'INTERVALNAME': ['Upper', 'Cretaceous'], 'idx': [22, 23]},\n",
       " 'TAXA': {'TAXA': ['Exidmonea'], 'idx': [7]},\n",
       " 'docid': '5ab3d0c3cf58f10e4a1e0ea5',\n",
       " 'sdp': {'idx': [8, 24, 13, 16, 23],\n",
       "  'words': ['Exidmonea', 'Cretaceous', 'redescription', 'dorsata', 'Upper']},\n",
       " 'sentence': ['Taxanomic',\n",
       "  'status',\n",
       "  'of',\n",
       "  'the',\n",
       "  'cyclostomes',\n",
       "  'bryozoans',\n",
       "  'genus',\n",
       "  'Exidmonea',\n",
       "  ',',\n",
       "  'with',\n",
       "  'with',\n",
       "  'a',\n",
       "  'redescription',\n",
       "  'of',\n",
       "  'E.',\n",
       "  'dorsata',\n",
       "  '-LRB-',\n",
       "  'von',\n",
       "  'Hagenow',\n",
       "  '-RRB-',\n",
       "  'from',\n",
       "  'the',\n",
       "  'Upper',\n",
       "  'Cretaceous',\n",
       "  '.'],\n",
       " 'sentid': 373}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
